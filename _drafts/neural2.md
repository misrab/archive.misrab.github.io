---
title: Neural Networks (2) - Basics
layout: blog
excerpt: |
  In the second part of this series, I look at the fundamental 
  concept of variation
---


# Neural Networks - Basics

## Introduction

In our last post we spoke about the general context of neural networks,
namely machine learning algorithms. The problem being solved is one of abstracting
rule definitions in solving a problem away from the programmer.

We also noted that neural networks might stand out in their ability to identify 
useful feature transformations automatically, which would serve this overarching
purpose.

Here I'll describe the fundamentals of neural network architecture, and why 
this architecture might be useful.

## Universality

Let start by examining what a neural network can theoretically do. Remember, the problem at hand is to take "some" input, and say something useful about it. We might want to 
group some inputs together, for example. In the extreme, most fine-grained case, we might 
want to distinguish between _every_ possible input. All other problems are a subset of this extreme case. So let's see if a neural network can do this.

## Non-linearity


## Notation

## Layers

## Gradient Problems


## Conclusion

## Acknowledgements

This series of posts is heavily inspired by the writings of 
[Michael Nielsen](http://neuralnetworksanddeeplearning.com/), 
[Richard Socher](http://www.socher.org/index.php/Main/HomePage), and 
[Christopher Olah](http://colah.github.io/), to whom much credit is due. Some of the 
statistical intuition is also influenced by [Trevor Hastie and Rob Tibshirani](http://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/).