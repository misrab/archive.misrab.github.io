---
title: Neural Networks (2) - Basics
layout: blog
excerpt: |
  In the second part of this series, I look at the fundamental 
  concept of variation
---


# Neural Networks - Basics

## Introduction

In our last post we spoke about the general context of neural networks,
namely machine learning algorithms. The problem being solved is one of abstracting
rule definitions in solving a problem.

We also noted that neural networks might stand out in their ability to identify 
useful feature transformations, which would serve the purpose of our desired 
"abstraction" after all.

Here I'll describe the fundamentals of neural network architecture, and why 
this architecture might be useful.

## Architecture



  - include nonlinearity here

## Layers

## Gradient Problems


## Conclusion

## Acknowledgements

This series of posts is heavily inspired by the writings of 
[Michael Nielsen](http://neuralnetworksanddeeplearning.com/), 
[Richard Socher](http://www.socher.org/index.php/Main/HomePage), and 
[Christopher Olah](http://colah.github.io/), to whom much credit is due. Some of the 
statistical intuition is also influenced by [Trevor Hastie and Rob Tibshirani](http://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/).